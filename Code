import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from numpy import sqrt
from sklearn.metrics import mean_squared_error, r2_score
from pmdarima.arima import auto_arima
from statsmodels.tsa.arima.model import ARIMA
import xgboost as xgb
from sklearn.ensemble import GradientBoostingRegressor

# Define forecasting functions
def sarima_forecast(data, seasonal_period=12, forecast_horizon=90, test_size=0.2):
    train_size = int(len(data) * (1 - test_size))
    train_data = data[:train_size]
    test_data = data[train_size:]

    auto_arima_model = auto_arima(train_data, seasonal=True, m=seasonal_period, trace=False, error_action='ignore',
                                  suppress_warnings=True)
    arima_model = ARIMA(train_data, order=auto_arima_model.order, seasonal_order=auto_arima_model.seasonal_order)
    arima_result = arima_model.fit()

    start_date = test_data.index[0]
    end_date = test_data.index[-1]
    predictions = arima_result.predict(start=start_date, end=end_date)

    rmse = sqrt(mean_squared_error(test_data, predictions))
    r2 = r2_score(test_data, predictions)

    return rmse, r2, predictions, test_data

def xgboost_forecast(data, lag_features=8, test_size=0.2):
    def get_lag(data, col, lagtime):
        for i in range(1, lagtime + 1):
            data[f"{col}_lag{i}"] = data[col].shift(i)
        return data

    data = data.to_frame(name="target")
    data = get_lag(data, "target", lag_features)
    data.dropna(inplace=True)

    split_date = int(len(data) * (1 - test_size))
    train_data = data.iloc[:split_date]
    test_data = data.iloc[split_date:]

    X_train, y_train = train_data.drop("target", axis=1), train_data["target"]
    X_test, y_test = test_data.drop("target", axis=1), test_data["target"]

    reg = xgb.XGBRegressor(n_estimators=1000, early_stopping_rounds=50, verbosity=0)
    reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)

    predictions = reg.predict(X_test)

    rmse = sqrt(mean_squared_error(y_test, predictions))
    r2 = r2_score(y_test, predictions)

    return rmse, r2, predictions, test_data

def gradient_boosting_forecast(data, lag_features=8, test_size=0.2):
    def get_lag(data, col, lagtime):
        for i in range(1, lagtime + 1):
            data[f"{col}_lag{i}"] = data[col].shift(i)
        return data

    data = data.to_frame(name="target")
    data = get_lag(data, "target", lag_features)
    data.dropna(inplace=True)

    split_date = int(len(data) * (1 - test_size))
    train_data = data.iloc[:split_date]
    test_data = data.iloc[split_date:]

    X_train, y_train = train_data.drop("target", axis=1), train_data["target"]
    X_test, y_test = test_data.drop("target", axis=1), test_data["target"]

    gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1, max_depth=3, random_state=42)
    gbr.fit(X_train, y_train)

    predictions = gbr.predict(X_test)

    rmse = sqrt(mean_squared_error(y_test, predictions))
    r2 = r2_score(y_test, predictions)

    return rmse, r2, predictions, test_data

# Streamlit UI
st.title("Energy Consumption Forecast")
st.sidebar.title("Model Selection")

@st.cache_data
def load_data(uploaded_file):
    consumption_df = pd.read_csv(uploaded_file, low_memory=False)

    consumption_df.columns = list(map(str.lower, consumption_df.columns))
    consumption_df['timestamp'] = pd.to_datetime(consumption_df['date'] + consumption_df['time'],
                                                 format='%d/%m/%Y%H:%M:%S', errors='coerce')
    consumption_df.drop(['date', 'time'], axis=1, inplace=True)

    float_cols = ['global_active_power', 'global_reactive_power', 'voltage',
                  'global_intensity', 'sub_metering_1', 'sub_metering_2',
                  'sub_metering_3']
    for col in float_cols:
        consumption_df[col] = pd.to_numeric(consumption_df[col], errors='coerce')

    consumption_df.set_index('timestamp', inplace=True)

    daily_consumption_df = consumption_df.resample(rule='D').mean()
    daily_consumption_df.ffill(inplace=True)
    monthly_consumption_df = daily_consumption_df.resample(rule='ME').mean()

    return monthly_consumption_df['global_active_power']

# File upload
uploaded_file = st.sidebar.file_uploader("Upload your dataset (CSV format)", type=["csv"])
if uploaded_file is not None:
    data = load_data(uploaded_file)
    st.write("Dataset loaded successfully!")
    st.write(data.head())

    # Model selection
    model_choice = st.sidebar.selectbox("Choose a model:", ("SARIMA", "XGBoost", "Gradient Boosting"))
    train_button = st.sidebar.button("Train Model")

    if train_button:
        st.write(f"Training {model_choice} model...")
        if model_choice == "SARIMA":
            rmse, r2, predictions, test_data = sarima_forecast(data)
        elif model_choice == "XGBoost":
            rmse, r2, predictions, test_data = xgboost_forecast(data)
        elif model_choice == "Gradient Boosting":
            rmse, r2, predictions, test_data = gradient_boosting_forecast(data)

        # Display results
        st.write(f"RMSE: {rmse:.2f}")
        st.write(f"R2: {r2:.2f}")

        # Plot results
        plt.figure(figsize=(10, 6))
        plt.plot(data.index, data, marker='o', label='Actual')
        plt.plot(test_data.index, predictions, color='red', marker='o', label='Predicted (Test)')
        plt.axvline(x=test_data.index[0], color='gray', linestyle='--', label='Train-Test Split')
        plt.legend()
        st.pyplot(plt)
